{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1b295b",
   "metadata": {},
   "source": [
    "# Trying another method\n",
    "\n",
    "Based on this tutorial: https://huggingface.co/blog/fine-tune-wav2vec2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa959f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lora_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "from datasets import Dataset, Audio\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea910c",
   "metadata": {},
   "source": [
    "## Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fae739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728 total files.\n",
      "1591 files were included (both training and evaluation).\n",
      "1192 files are used for training.\n",
      "399 files are used fr evaluation.\n",
      "137 files were excluded due to incorrect word.\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Load participant sets CSV\n",
    "participant_df = pd.read_csv(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/participant_sets.csv\")\n",
    "training_ids = participant_df[participant_df['set'].str.lower() == 'training']['ParticipantID'].str.lower().tolist()\n",
    "training_ids = [id.strip().lower() for id in training_ids]\n",
    "eval_ids = participant_df[participant_df['set'].str.lower() == 'evaluation']['ParticipantID'].str.lower().tolist()\n",
    "\n",
    "# Load csv of incorrect utterances\n",
    "# Create set of included files for training\n",
    "inclusion_df = pd.read_csv(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/incorrect_utterances.csv\")\n",
    "incorrect_word_files = inclusion_df[inclusion_df['Subject inclusion'].str.lower() == 'include']['Filename'].tolist()\n",
    "incorrect_word_files = set([f.lower() for f in incorrect_word_files])  # normalize case\n",
    "\n",
    "# Extract filepath, participant info, and word from KT1 data file directory\n",
    "base_dir = \"/home/cogsci-lasrlab/Documents/CSS_Capstone/KT1_data\"\n",
    "data = []\n",
    "included_count = 0\n",
    "excluded_count = 0\n",
    "num_files = 0\n",
    "num_train = 0\n",
    "num_eval = 0\n",
    "\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.wav') and \"participant_\" in file:\n",
    "                base = file[:-4]  # remove '.wav'\n",
    "                before, word = base.split(\"participant_\")\n",
    "                    \n",
    "                # Remove 'K1' and researcher number before 'participant_'\n",
    "                ppt_id = before.replace(\"K1\", \"\")[:-1]\n",
    "                ppt_id_clean = ppt_id.strip().lower() \n",
    "                \n",
    "                # handle missing participant\n",
    "                if \"rri0\" in file.lower():\n",
    "                    ppt_id_clean = \"rri0\"\n",
    "                num_files += 1\n",
    "\n",
    "                # Check if file has correct word\n",
    "                if file.lower() not in incorrect_word_files:\n",
    "                    included_count += 1\n",
    "                    # Only include training data\n",
    "                    if ppt_id_clean.lower() in training_ids:\n",
    "                        num_train += 1\n",
    "                        data.append({\n",
    "                            \"file_name\": os.path.join(folder_path, file),\n",
    "                            \"ppt_id\": ppt_id,\n",
    "                            \"transcription\": word\n",
    "                            })\n",
    "                                \n",
    "                    #Count number of excluded file\n",
    "                    else:\n",
    "                        num_eval += 1\n",
    "                else:\n",
    "                    excluded_count += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Report\n",
    "print(f\"{num_files} total files.\") # should be 1728\n",
    "print(f\"{included_count} files were included (both training and evaluation).\") # should be 1591\n",
    "print(f\"{num_train} files are used for training.\") # should be 1192\n",
    "print(f\"{num_eval} files are used fr evaluation.\") # should be 399\n",
    "print(f\"{excluded_count} files were excluded due to incorrect word.\") # should be 137\n",
    "\n",
    "# Convert to dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Clean structure for downstram preprocessing\n",
    "dataset = dataset.rename_column(\"transcription\", \"text\") \n",
    "dataset = dataset.rename_column(\"file_name\", \"audio\") \n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# Train-test split\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed723e2",
   "metadata": {},
   "source": [
    "## Create custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b289f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 894/894 [00:00<00:00, 59537.13 examples/s]\n",
      "Map: 100%|██████████| 298/298 [00:00<00:00, 57841.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"text\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocabs = dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=dataset.column_names[\"train\"])\n",
    "\n",
    "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6667631",
   "metadata": {},
   "source": [
    "## Initialize feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4351612",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34cf08",
   "metadata": {},
   "source": [
    "## Wrap feature extractor and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "534852c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc15129",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baf6373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    \n",
    "    batch[\"labels\"] = processor(text=batch[\"text\"]).input_ids\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c35f91dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 953/953 [00:02<00:00, 413.31 examples/s]\n",
      "Map: 100%|██████████| 239/239 [00:00<00:00, 427.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab0dec",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "### Initialize CTC Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1c46675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1888632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e294218",
   "metadata": {},
   "source": [
    "### Try fine-tuning wav2vec (without LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac6d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2175: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Wav2Vec2 model\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# Freeze feature extractor layer\n",
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbdafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/finetuned_wav2vec2\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=5,\n",
    "  eval_strategy=\"no\",\n",
    "  num_train_epochs=1,\n",
    "  fp16=False,\n",
    "  gradient_checkpointing=True, \n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=10,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.0,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    "    label_names=['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b249a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_209626/549408762.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00305ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='191' max='191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [191/191 07:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>44.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>32.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>27.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>23.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>27.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>33.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>19.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>11.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>4.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>4.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.332200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.673100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=191, training_loss=14.087261709243215, metrics={'train_runtime': 453.5991, 'train_samples_per_second': 2.101, 'train_steps_per_second': 0.421, 'total_flos': 7005069726364800.0, 'train_loss': 14.087261709243215, 'epoch': 1.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1003ef8",
   "metadata": {},
   "source": [
    "### Inject LoRA into Wav2Vec2 Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bf947ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 94,691,232 || trainable%: 0.3114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2175: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# initialize base model\n",
    "# base_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "base_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Configure LoRA arguments\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # can change rank, r=8 is most common\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # attention layers\n",
    ")\n",
    "\n",
    "# Add LoRA layers to Wav2Vec2 model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Do not train feature extractor\n",
    "model.freeze_feature_extractor()\n",
    "\n",
    "# Print number and proportion of trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409bb3c",
   "metadata": {},
   "source": [
    "### Initialize Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d85f87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/lora_test\", # use absolute path\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     eval_strategy=\"no\",\n",
    "#     save_strategy=\"no\",\n",
    "#     learning_rate=5e-4,\n",
    "#     weight_decay=0.01, # L2 regularization, lower if the loss \n",
    "#     num_train_epochs=1, # make this larger\n",
    "#     fp16=False,\n",
    "#     gradient_accumulation_steps=5, # experiment with making this larger (larger = faster)\n",
    "#     # logging_dir=\"./logs_test\",\n",
    "#     logging_steps=10,\n",
    "#     report_to=\"none\",\n",
    "#     label_names=[\"labels\"]\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/lora_test\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=5,\n",
    "  eval_strategy=\"no\",\n",
    "  num_train_epochs=1,\n",
    "  fp16=False,\n",
    "  gradient_checkpointing=True, \n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=10,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.0,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    "    label_names=['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f030645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipBatchTrainer(Trainer):\n",
    "    def train_dataloader(self):\n",
    "        dataloader = super().get_train_dataloader()\n",
    "        return self.__skip_error_dataloader()\n",
    "    \n",
    "    def skip_error_dataloader(self, dataloader):\n",
    "        for batch in dataloader:\n",
    "            try:\n",
    "                yield batch\n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Skipping batch due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        try:\n",
    "            return super().training_step(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Forward pass failed, skipping batch: {e}\")\n",
    "            return torch.tensor(0.0).to(self.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3f42f5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203099/260261800.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SkipBatchTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SkipBatchTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = SkipBatchTrainer(\n",
    "    model=model,\n",
    "    data_collator = data_collator,\n",
    "    args=training_args,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a66a8",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3e5543c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Forward pass failed, skipping batch: element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/179 00:08 < 03:17, 0.86 it/s, Epoch 0.04/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Forward pass failed, skipping batch: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "[WARNING] Forward pass failed, skipping batch: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "[WARNING] Forward pass failed, skipping batch: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "[WARNING] Forward pass failed, skipping batch: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "[WARNING] Forward pass failed, skipping batch: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "[WARNING] Forward pass failed, skipping batch: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "[WARNING] Forward pass failed, skipping batch: element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(\"Done training.\")\n",
    "model.save_pretrained(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/lora_test\") # use absolute path\n",
    "processor.save_pretrained(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/lora_test\") # use absolute path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3c98d",
   "metadata": {},
   "source": [
    "## NEXT STEPS\n",
    "\n",
    "- MAKE `vocab.json` ALL CAPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e284c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
