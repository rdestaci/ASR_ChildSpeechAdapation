{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torchaudio\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ASR implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "path = \"/Users/reekaestacio/Documents/GitHub/Whisper_LLM_Bias/data\"\n",
    "df = pd.read_csv(path+\"/stimuli.csv\")\n",
    "df[\"file_name\"] = path + \"/auditory_stimuli/\" + df[\"id\"].astype(str) + df[\"condition_id\"] + \".mp3\"\n",
    "\n",
    "# Load pretrained model and processor\n",
    "model_name = \"facebook/wav2vec2-base-960h\"  # or your fine-tuned model path\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_mp3(file_path, target_sr=16000):\n",
    "    # Load and resample audio\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    if sample_rate != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Mono-channel and flatten\n",
    "    input_values = processor(waveform.squeeze(), sampling_rate=target_sr, return_tensors=\"pt\").input_values\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode to text\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model\n",
    "df[\"prediction\"] = df[\"file_name\"].apply(lambda x: transcribe_mp3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 incorrect predictions.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>condition_id</th>\n",
       "      <th>condition_name</th>\n",
       "      <th>sentence</th>\n",
       "      <th>file_name</th>\n",
       "      <th>prediction</th>\n",
       "      <th>accurate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>expected</td>\n",
       "      <td>THE CHEF SHARPENED THE KNIFE</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE CHEFF SHARPENED THE KNIFE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>B</td>\n",
       "      <td>phonologically related</td>\n",
       "      <td>THE CHEF SHARPENED THE NIGHT</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE CHEFF SHARPENED THE NIGHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "      <td>semantically related</td>\n",
       "      <td>THE CHEF SHARPENED THE BLADE</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE CHEFF SHARPENED THE BLADE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>D</td>\n",
       "      <td>both</td>\n",
       "      <td>THE CHEF SHARPENED THE KNIVES</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE CHEFF SHARPENED THE KNIVES</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>E</td>\n",
       "      <td>neither</td>\n",
       "      <td>THE CHEF SHARPENED THE SUN</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE CHEFF SHARPENED THE SUN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>expected</td>\n",
       "      <td>THE LUMBERJACK CHOPPED THE TREE</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE LUMBER JACK CHOPPED THE TREE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6</td>\n",
       "      <td>B</td>\n",
       "      <td>phonologically related</td>\n",
       "      <td>THE LUMBERJACK CHOPPED THE TREAT</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE LUMBER JACK CHOPPED THE TREAT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "      <td>semantically related</td>\n",
       "      <td>THE LUMBERJACK CHOPPED THE LOG</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE LUMBER JACK CHOPPED THE LOG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>D</td>\n",
       "      <td>both</td>\n",
       "      <td>THE LUMBERJACK CHOPPED THE TRUNK</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE LUMBER JACK CHOPPED THE TRUNK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>E</td>\n",
       "      <td>neither</td>\n",
       "      <td>THE LUMBERJACK CHOPPED THE COFFEE</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE LUMBER JACK CHOPPED THE COFFEE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7</td>\n",
       "      <td>B</td>\n",
       "      <td>phonologically related</td>\n",
       "      <td>THE BABY DRANK THE MELT</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE BABY DRANK THE MILT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8</td>\n",
       "      <td>D</td>\n",
       "      <td>both</td>\n",
       "      <td>THE DOG BURIED THE BOWL</td>\n",
       "      <td>/Users/reekaestacio/Documents/GitHub/Whisper_L...</td>\n",
       "      <td>THE DOG BURIED THE BUWL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id condition_id          condition_name  \\\n",
       "20   5            A                expected   \n",
       "21   5            B  phonologically related   \n",
       "22   5            C    semantically related   \n",
       "23   5            D                    both   \n",
       "24   5            E                 neither   \n",
       "25   6            A                expected   \n",
       "26   6            B  phonologically related   \n",
       "27   6            C    semantically related   \n",
       "28   6            D                    both   \n",
       "29   6            E                 neither   \n",
       "31   7            B  phonologically related   \n",
       "38   8            D                    both   \n",
       "\n",
       "                             sentence  \\\n",
       "20       THE CHEF SHARPENED THE KNIFE   \n",
       "21       THE CHEF SHARPENED THE NIGHT   \n",
       "22       THE CHEF SHARPENED THE BLADE   \n",
       "23      THE CHEF SHARPENED THE KNIVES   \n",
       "24         THE CHEF SHARPENED THE SUN   \n",
       "25    THE LUMBERJACK CHOPPED THE TREE   \n",
       "26   THE LUMBERJACK CHOPPED THE TREAT   \n",
       "27     THE LUMBERJACK CHOPPED THE LOG   \n",
       "28   THE LUMBERJACK CHOPPED THE TRUNK   \n",
       "29  THE LUMBERJACK CHOPPED THE COFFEE   \n",
       "31            THE BABY DRANK THE MELT   \n",
       "38            THE DOG BURIED THE BOWL   \n",
       "\n",
       "                                            file_name  \\\n",
       "20  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "21  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "22  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "23  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "24  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "25  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "26  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "27  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "28  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "29  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "31  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "38  /Users/reekaestacio/Documents/GitHub/Whisper_L...   \n",
       "\n",
       "                            prediction  accurate  \n",
       "20       THE CHEFF SHARPENED THE KNIFE         0  \n",
       "21       THE CHEFF SHARPENED THE NIGHT         0  \n",
       "22       THE CHEFF SHARPENED THE BLADE         0  \n",
       "23      THE CHEFF SHARPENED THE KNIVES         0  \n",
       "24         THE CHEFF SHARPENED THE SUN         0  \n",
       "25    THE LUMBER JACK CHOPPED THE TREE         0  \n",
       "26   THE LUMBER JACK CHOPPED THE TREAT         0  \n",
       "27     THE LUMBER JACK CHOPPED THE LOG         0  \n",
       "28   THE LUMBER JACK CHOPPED THE TRUNK         0  \n",
       "29  THE LUMBER JACK CHOPPED THE COFFEE         0  \n",
       "31             THE BABY DRANK THE MILT         0  \n",
       "38             THE DOG BURIED THE BUWL         0  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize labels\n",
    "# w2v2 labels with uppercase\n",
    "df['sentence'] = df['sentence'].str.upper()\n",
    "df['sentence'] = df['sentence'].str.replace(\".\", \"\", regex=False)\n",
    "\n",
    "\n",
    "\n",
    "num_incorrect = 0\n",
    "accurate_list = []\n",
    "\n",
    "# labels accurate/inaccurate\n",
    "for _, row in df.iterrows():\n",
    "    if row[\"sentence\"] == row[\"prediction\"]:\n",
    "        accurate_list.append(1)\n",
    "    else:\n",
    "        accurate_list.append(0)\n",
    "        num_incorrect += 1\n",
    "\n",
    "df[\"accurate\"] = accurate_list\n",
    "print(f\"{num_incorrect} incorrect predictions.\")\n",
    "\n",
    "# see inaccuracies\n",
    "incorrect_df = df[df['accurate']==0]\n",
    "incorrect_df\n",
    "\n",
    "## spelling differences in wav2vec, not a problem for real dataset because of simple, single word utterances\n",
    "## only 2 real mislabels, better to convert to phones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# import soundfile\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_df = df[['file_name', 'sentence']]\n",
    "dataset = Dataset.from_pandas(lora_df)\n",
    "# Train-test split\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(example[\"file_name\"])\n",
    "    \n",
    "    # Resample\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Flatten list of waveforms for correct dimensionality\n",
    "    waveform = waveform.squeeze(0)  # shape becomes [samples]\n",
    "\n",
    "    # Use the feature extractor (not full processor) — output is a dict with lists\n",
    "    inputs = processor.feature_extractor(\n",
    "        waveform,\n",
    "        sampling_rate=16000,\n",
    "        padding=False, # ensures files are the same size\n",
    "        return_attention_mask=True, # tells the model what is actually audio and what is padding\n",
    "        return_tensors=None  \n",
    "    )\n",
    "\n",
    "    # Flatten input_values from [[...]] → [...] for correct dimensionality\n",
    "    input_values = inputs[\"input_values\"][0]\n",
    "    attention_mask = inputs[\"attention_mask\"][0]\n",
    "\n",
    "    # Tokenize word transcripts (label_ids)\n",
    "    with processor.as_target_processor():\n",
    "        label_ids = processor.tokenizer(\n",
    "            example[\"sentence\"].upper(), # upper to ensure correct tokenization\n",
    "            return_tensors=None,\n",
    "            padding=False,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "\n",
    "    return {\n",
    "        # keep all inputs lists\n",
    "        \"input_values\": input_values[0],\n",
    "        \"attention_mask\": attention_mask[0],\n",
    "        \"labels\": label_ids[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/32 [00:00<?, ? examples/s]/opt/anaconda3/envs/lora_env/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 32/32 [00:00<00:00, 582.47 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 767.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset_train= dataset['train'].map(preprocess, remove_columns=[\"file_name\", \"sentence\"])\n",
    "processed_dataset_test= dataset['test'].map(preprocess, remove_columns=[\"file_name\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 6,\n",
       " 12,\n",
       " 11,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 11,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 11,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 11,\n",
       " 6,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset_train['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 94,691,232 || trainable%: 0.3114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lora_env/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2175: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Wav2Vec model and processor\n",
    "# Instantiate Wav2Vec2 model\n",
    "base_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Configure LoRA arguments\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # can change rank, r=8 is most common\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # attention layers\n",
    ")\n",
    "\n",
    "# Add LoRA layers to Wav2Vec2 model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.freeze_feature_extractor()\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/3gk3p4nn5tlcdvb5zkgy4xh40000gn/T/ipykernel_47453/1000897533.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Initialize training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./lora_w2v2_test\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=5,\n",
    "  eval_strategy=\"no\",\n",
    "  num_train_epochs=3,\n",
    "  fp16=False,\n",
    "  gradient_checkpointing=True, \n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=10,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.0,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    "  label_names=['labels'],\n",
    "  remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Instantiate Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset_train,\n",
    "    eval_dataset=processed_dataset_test,\n",
    "    tokenizer=processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lora_env/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lora_env/lib/python3.10/site-packages/transformers/trainer.py:2274\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2272\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 2274\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   2276\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lora_env/lib/python3.10/site-packages/transformers/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1017\u001b[0m dataloader_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size,\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollate_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersistent_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_persistent_workers,\n\u001b[1;32m   1023\u001b[0m }\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterableDataset):\n\u001b[0;32m-> 1026\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_train_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_drop_last\n\u001b[1;32m   1028\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lora_env/lib/python3.10/site-packages/transformers/trainer.py:988\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    984\u001b[0m         lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     model_input_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     )\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLengthGroupedSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RandomSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lora_env/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:644\u001b[0m, in \u001b[0;36mLengthGroupedSampler.__init__\u001b[0;34m(self, batch_size, dataset, lengths, model_input_name, generator)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], BatchEncoding))\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    639\u001b[0m     ):\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    641\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    642\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[0;32m--> 644\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(feature[model_input_name]) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lengths, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    646\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lora_env/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:644\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], BatchEncoding))\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    639\u001b[0m     ):\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    641\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    642\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[0;32m--> 644\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lengths, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    646\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
