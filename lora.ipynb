{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# import soundfile\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Charsiu model\n",
    "charsiu_dir = '/home/cogsci-lasrlab/ForcedAlignment/programs/charsiu-main'\n",
    "os.chdir(charsiu_dir)\n",
    "\n",
    "sys.path.append('%s/src/' % charsiu_dir)\n",
    "\n",
    "from Charsiu import charsiu_forced_aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Load participant sets CSV\n",
    "participant_df = pd.read_csv(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/participant_sets.csv\")\n",
    "training_ids = participant_df[participant_df['set'].str.lower() == 'training']['ParticipantID'].str.lower().tolist()\n",
    "training_ids = [id.strip().lower() for id in training_ids]\n",
    "eval_ids = participant_df[participant_df['set'].str.lower() == 'evaluation']['ParticipantID'].str.lower().tolist()\n",
    "\n",
    "# Load csv of incorrect utterances\n",
    "# Create set of included files for training\n",
    "inclusion_df = pd.read_csv(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/incorrect_utterances.csv\")\n",
    "incorrect_word_files = inclusion_df[inclusion_df['Subject inclusion'].str.lower() == 'include']['Filename'].tolist()\n",
    "incorrect_word_files = set([f.lower() for f in incorrect_word_files])  # normalize case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728 total files.\n",
      "1591 files were included (both training and evaluation).\n",
      "1192 files are used for training.\n",
      "399 files are used fr evaluation.\n",
      "137 files were excluded due to incorrect word.\n"
     ]
    }
   ],
   "source": [
    "# extract filepath, participant info, and word from KT1 data\n",
    "base_dir = \"/home/cogsci-lasrlab/Documents/CSS_Capstone/KT1_data\"\n",
    "data = []\n",
    "included_count = 0\n",
    "excluded_count = 0\n",
    "num_files = 0\n",
    "num_train = 0\n",
    "num_eval = 0\n",
    "\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.wav') and \"participant_\" in file:\n",
    "                base = file[:-4]  # remove '.wav'\n",
    "                before, word = base.split(\"participant_\")\n",
    "                    \n",
    "                # Remove 'K1' and researcher number before 'participant_'\n",
    "                ppt_id = before.replace(\"K1\", \"\")[:-1]\n",
    "                ppt_id_clean = ppt_id.strip().lower() \n",
    "                \n",
    "                # handle missing participant\n",
    "                if \"rri0\" in file.lower():\n",
    "                    ppt_id_clean = \"rri0\"\n",
    "                num_files += 1\n",
    "\n",
    "                # Check if file has correct word\n",
    "                if file.lower() not in incorrect_word_files:\n",
    "                    included_count += 1\n",
    "                    # Only include training data\n",
    "                    if ppt_id_clean.lower() in training_ids:\n",
    "                        num_train += 1\n",
    "                        data.append({\n",
    "                            \"file_name\": os.path.join(folder_path, file),\n",
    "                            \"ppt_id\": ppt_id,\n",
    "                            \"transcription\": word\n",
    "                            })\n",
    "                            \n",
    "                            # Write .txt file\n",
    "#                             txt_path = os.path.join(folder_path, f\"{os.path.splitext(file)[0]}.txt\")\n",
    "#                             with open(txt_path, 'w') as f:\n",
    "#                                 f.write(word)\n",
    "                                \n",
    "                    #Count number of excluded file\n",
    "                    else:\n",
    "                        num_eval += 1\n",
    "                else:\n",
    "                    excluded_count += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Report\n",
    "print(f\"{num_files} total files.\") # should be 1728\n",
    "print(f\"{included_count} files were included (both training and evaluation).\") # should be 1591\n",
    "print(f\"{num_train} files are used for training.\") # should be 1192\n",
    "print(f\"{num_eval} files are used fr evaluation.\") # should be 399\n",
    "print(f\"{excluded_count} files were excluded due to incorrect word.\") # should be 137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Wav2Vec model and processor\n",
    "base_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 94,691,232 || trainable%: 0.3114\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA arguments\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # can change rank, r=8 is most common\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # attention layers\n",
    ")\n",
    "\n",
    "# Add LoRA layers to Wav2Vec2 model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(example[\"file_name\"])\n",
    "    \n",
    "    # Resample\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Flatten list of waveforms for correct dimensionality\n",
    "    waveform = waveform.squeeze(0)  # shape becomes [samples]\n",
    "\n",
    "    # Use the feature extractor (not full processor) — output is a dict with lists\n",
    "    inputs = processor.feature_extractor(\n",
    "        waveform,\n",
    "        sampling_rate=16000,\n",
    "        padding=True, # ensures files are the same size\n",
    "        return_attention_mask=True, # tells the model what is actually audio and what is padding\n",
    "        return_tensors=None  \n",
    "    )\n",
    "\n",
    "    # Flatten input_values from [[...]] → [...] for correct dimensionality\n",
    "    input_values = inputs[\"input_values\"][0]\n",
    "    attention_mask = inputs[\"attention_mask\"][0]\n",
    "\n",
    "    # Tokenize word transcripts (label_ids)\n",
    "    with processor.as_target_processor():\n",
    "        label_ids = processor.tokenizer(\n",
    "            example[\"transcription\"],\n",
    "            return_tensors=None,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "\n",
    "    return {\n",
    "        # keep all inputs lists\n",
    "        \"input_values\": input_values,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": label_ids  # something wrong here, tokenizing wrong\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1192 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1192/1192 [00:17<00:00, 68.44 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data from pandas dataframe to transformers dataset\n",
    "# Preprocesses data for training\n",
    "dataset = Dataset.from_pandas(df)\n",
    "processed_dataset = dataset.map(preprocess, remove_columns=[\"file_name\", \"ppt_id\", \"transcription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': [-0.019328845664858818,\n",
       "  -0.02364995703101158,\n",
       "  -0.025950400158762932,\n",
       "  -0.028182629495859146,\n",
       "  -0.034165073186159134,\n",
       "  -0.02425319328904152,\n",
       "  -0.013984720222651958,\n",
       "  -0.026494089514017105,\n",
       "  -0.03734378516674042,\n",
       "  -0.032913267612457275,\n",
       "  -0.02996821701526642,\n",
       "  -0.027966538444161415,\n",
       "  -0.033311568200588226,\n",
       "  -0.037378568202257156,\n",
       "  -0.03392801806330681,\n",
       "  -0.028891323134303093,\n",
       "  -0.025521257892251015,\n",
       "  -0.020201221108436584,\n",
       "  -0.016860103234648705,\n",
       "  -0.016673976555466652,\n",
       "  -0.018833322450518608,\n",
       "  -0.028969548642635345,\n",
       "  -0.03109755367040634,\n",
       "  -0.026445642113685608,\n",
       "  -0.024979209527373314,\n",
       "  -0.021155143156647682,\n",
       "  -0.025270890444517136,\n",
       "  -0.02020949497818947,\n",
       "  -0.02433164231479168,\n",
       "  -0.018452046439051628,\n",
       "  -0.029209939762949944,\n",
       "  -0.04168390482664108,\n",
       "  -0.03598972409963608,\n",
       "  -0.03202756866812706,\n",
       "  -0.03900420665740967,\n",
       "  -0.0339745469391346,\n",
       "  -0.034425172954797745,\n",
       "  -0.03980156034231186,\n",
       "  -0.03502005711197853,\n",
       "  -0.03623325005173683,\n",
       "  -0.03480100631713867,\n",
       "  -0.03301620855927467,\n",
       "  -0.035543084144592285,\n",
       "  -0.024761440232396126,\n",
       "  -0.03218454122543335,\n",
       "  -0.027584346011281013,\n",
       "  -0.02157627046108246,\n",
       "  -0.02378966100513935,\n",
       "  -0.0275331512093544,\n",
       "  -0.033246129751205444,\n",
       "  -0.02845156192779541,\n",
       "  -0.020047420635819435,\n",
       "  -0.018521320074796677,\n",
       "  -0.02656427212059498,\n",
       "  -0.0243512075394392,\n",
       "  -0.03946318477392197,\n",
       "  -0.02874201536178589,\n",
       "  -0.02644357830286026,\n",
       "  -0.03314633667469025,\n",
       "  -0.03156132623553276,\n",
       "  -0.033003874123096466,\n",
       "  -0.023937243968248367,\n",
       "  -0.024773215875029564,\n",
       "  -0.03961743786931038,\n",
       "  -0.0455358661711216,\n",
       "  -0.030103234574198723,\n",
       "  -0.03996986523270607,\n",
       "  -0.027074623852968216,\n",
       "  -0.03035859204828739,\n",
       "  -0.03110283426940441,\n",
       "  -0.03155893459916115,\n",
       "  -0.03018476814031601,\n",
       "  -0.03333469480276108,\n",
       "  -0.027609650045633316,\n",
       "  -0.03324568644165993,\n",
       "  -0.028621463105082512,\n",
       "  -0.03553187474608421,\n",
       "  -0.03537916764616966,\n",
       "  -0.029466325417160988,\n",
       "  -0.03210306912660599,\n",
       "  -0.029359985142946243,\n",
       "  -0.028949186205863953,\n",
       "  -0.029273727908730507,\n",
       "  -0.02566375583410263,\n",
       "  -0.03177928924560547,\n",
       "  -0.03774264454841614,\n",
       "  -0.04058864340186119,\n",
       "  -0.04559404402971268,\n",
       "  -0.033493757247924805,\n",
       "  -0.04160253331065178,\n",
       "  -0.034384388476610184,\n",
       "  -0.031622108072042465,\n",
       "  -0.036845214664936066,\n",
       "  -0.0313398651778698,\n",
       "  -0.0336262509226799,\n",
       "  -0.03355402126908302,\n",
       "  -0.03034687601029873,\n",
       "  -0.03973475471138954,\n",
       "  -0.0365435853600502,\n",
       "  -0.03676983341574669,\n",
       "  -0.046521421521902084,\n",
       "  -0.04147849231958389,\n",
       "  -0.039046578109264374,\n",
       "  -0.047779131680727005,\n",
       "  -0.04459809139370918,\n",
       "  -0.03228968381881714,\n",
       "  -0.0409814827144146,\n",
       "  -0.03480016440153122,\n",
       "  -0.040195658802986145,\n",
       "  -0.03673466667532921,\n",
       "  -0.041241373866796494,\n",
       "  -0.045069340616464615,\n",
       "  -0.03647000715136528,\n",
       "  -0.037301816046237946,\n",
       "  -0.05296437442302704,\n",
       "  -0.05498761311173439,\n",
       "  -0.05257618799805641,\n",
       "  -0.04068433865904808,\n",
       "  -0.04002553969621658,\n",
       "  -0.0468958243727684,\n",
       "  -0.051395539194345474,\n",
       "  -0.05338820815086365,\n",
       "  -0.04770214483141899,\n",
       "  -0.04459061473608017,\n",
       "  -0.047679800540208817,\n",
       "  -0.052641794085502625,\n",
       "  -0.05715607851743698,\n",
       "  -0.060849204659461975,\n",
       "  -0.05438973382115364,\n",
       "  -0.04199240356683731,\n",
       "  -0.051333289593458176,\n",
       "  -0.06193528324365616,\n",
       "  -0.0637352392077446,\n",
       "  -0.04299939423799515,\n",
       "  -0.040222276002168655,\n",
       "  -0.04325011745095253,\n",
       "  -0.06176405027508736,\n",
       "  -0.05601269379258156,\n",
       "  -0.045562103390693665,\n",
       "  -0.050144847482442856,\n",
       "  -0.05670803040266037,\n",
       "  -0.06172073259949684,\n",
       "  -0.0603247806429863,\n",
       "  -0.0593319907784462,\n",
       "  -0.05884344503283501,\n",
       "  -0.05194335803389549,\n",
       "  -0.048074815422296524,\n",
       "  -0.06271619349718094,\n",
       "  -0.06077954173088074,\n",
       "  -0.061402007937431335,\n",
       "  -0.057916779071092606,\n",
       "  -0.06395313888788223,\n",
       "  -0.06711016595363617,\n",
       "  -0.0669049546122551,\n",
       "  -0.0666038766503334,\n",
       "  -0.06299078464508057,\n",
       "  -0.06391573697328568,\n",
       "  -0.07056178152561188,\n",
       "  -0.05545620620250702,\n",
       "  -0.062048543244600296,\n",
       "  -0.06528680771589279,\n",
       "  -0.06398411095142365,\n",
       "  -0.06875576823949814,\n",
       "  -0.05476796627044678,\n",
       "  -0.052728693932294846,\n",
       "  -0.05675629526376724,\n",
       "  -0.06536021083593369,\n",
       "  -0.06166361644864082,\n",
       "  -0.054117631167173386,\n",
       "  -0.0562235526740551,\n",
       "  -0.07046373188495636,\n",
       "  -0.0718611404299736,\n",
       "  -0.06132005900144577,\n",
       "  -0.05165484920144081,\n",
       "  -0.049086861312389374,\n",
       "  -0.05253535881638527,\n",
       "  -0.06382136046886444,\n",
       "  -0.0615021213889122,\n",
       "  -0.06285398453474045,\n",
       "  -0.042525772005319595,\n",
       "  -0.02615882083773613,\n",
       "  -0.07188133895397186,\n",
       "  -0.05095365643501282,\n",
       "  -0.039344798773527145,\n",
       "  -0.04161231219768524,\n",
       "  -0.056730665266513824,\n",
       "  -0.04755333065986633,\n",
       "  -0.04686171934008598,\n",
       "  -0.052442338317632675,\n",
       "  -0.05197044089436531,\n",
       "  -0.028089145198464394,\n",
       "  -0.0316326878964901,\n",
       "  -0.04616629704833031,\n",
       "  -0.05785346031188965,\n",
       "  -0.06828678399324417,\n",
       "  -0.015867874026298523,\n",
       "  -0.008831205777823925,\n",
       "  -0.06283604353666306,\n",
       "  -0.0601947084069252,\n",
       "  -0.03505577892065048,\n",
       "  -0.02828773856163025,\n",
       "  -0.03043864294886589,\n",
       "  -0.04139654338359833,\n",
       "  -0.02403196692466736,\n",
       "  -0.009945680387318134,\n",
       "  -0.02713577076792717,\n",
       "  -0.030219189822673798,\n",
       "  -0.02470526099205017,\n",
       "  -0.029035547748208046,\n",
       "  -0.049053482711315155,\n",
       "  -0.0335460901260376,\n",
       "  -0.03827021270990372,\n",
       "  -0.04120292887091637,\n",
       "  -0.03652891516685486,\n",
       "  -0.0386643148958683,\n",
       "  -0.055334240198135376,\n",
       "  -0.03878086060285568,\n",
       "  -0.03854725509881973,\n",
       "  -0.04612208530306816,\n",
       "  -0.04826011136174202,\n",
       "  -0.04451677203178406,\n",
       "  -0.04298298805952072,\n",
       "  -0.045167919248342514,\n",
       "  -0.04619517922401428,\n",
       "  -0.04516274482011795,\n",
       "  -0.05582147091627121,\n",
       "  -0.06584569066762924,\n",
       "  -0.06760261207818985,\n",
       "  -0.06500319391489029,\n",
       "  -0.05153290182352066,\n",
       "  -0.050307050347328186,\n",
       "  -0.0623992457985878,\n",
       "  -0.06688407808542252,\n",
       "  -0.06270024180412292,\n",
       "  -0.05670260637998581,\n",
       "  -0.04811439290642738,\n",
       "  -0.04951896890997887,\n",
       "  -0.060061611235141754,\n",
       "  -0.06004902347922325,\n",
       "  -0.060697879642248154,\n",
       "  -0.05285249650478363,\n",
       "  -0.06065187230706215,\n",
       "  -0.05312517285346985,\n",
       "  -0.055358629673719406,\n",
       "  -0.06595707684755325,\n",
       "  -0.07104305177927017,\n",
       "  -0.06430400162935257,\n",
       "  -0.04676530882716179,\n",
       "  -0.055705271661281586,\n",
       "  -0.06362615525722504,\n",
       "  -0.06856529414653778,\n",
       "  -0.0629190057516098,\n",
       "  -0.04714516922831535,\n",
       "  -0.05150742083787918,\n",
       "  -0.06835075467824936,\n",
       "  -0.07275190204381943,\n",
       "  -0.06857109069824219,\n",
       "  -0.05616256967186928,\n",
       "  -0.06731627881526947,\n",
       "  -0.06994368135929108,\n",
       "  -0.07753696292638779,\n",
       "  -0.07315947115421295,\n",
       "  -0.07284338027238846,\n",
       "  -0.08050809055566788,\n",
       "  -0.07916010916233063,\n",
       "  -0.0799417495727539,\n",
       "  -0.07069402933120728,\n",
       "  -0.07577365636825562,\n",
       "  -0.08590127527713776,\n",
       "  -0.08195361495018005,\n",
       "  -0.08550896495580673,\n",
       "  -0.08419065177440643,\n",
       "  -0.08622448146343231,\n",
       "  -0.09636500477790833,\n",
       "  -0.07751432061195374,\n",
       "  -0.09119104593992233,\n",
       "  -0.09075967967510223,\n",
       "  -0.09200675785541534,\n",
       "  -0.09043970704078674,\n",
       "  -0.08444371074438095,\n",
       "  -0.0878988727927208,\n",
       "  -0.09308020770549774,\n",
       "  -0.09851545095443726,\n",
       "  -0.08701211959123611,\n",
       "  -0.0773516595363617,\n",
       "  -0.08437785506248474,\n",
       "  -0.07934638857841492,\n",
       "  -0.0830054059624672,\n",
       "  -0.08815464377403259,\n",
       "  -0.09253290295600891,\n",
       "  -0.08668790757656097,\n",
       "  -0.08897656947374344,\n",
       "  -0.09399053454399109,\n",
       "  -0.09147844463586807,\n",
       "  -0.09009356051683426,\n",
       "  -0.0869162306189537,\n",
       "  -0.0947786495089531,\n",
       "  -0.10101078450679779,\n",
       "  -0.09247307479381561,\n",
       "  -0.09528345614671707,\n",
       "  -0.10004841536283493,\n",
       "  -0.09769041836261749,\n",
       "  -0.10946083813905716,\n",
       "  -0.1119529977440834,\n",
       "  -0.11758478730916977,\n",
       "  -0.13228128850460052,\n",
       "  -0.14280685782432556,\n",
       "  -0.16554594039916992,\n",
       "  -0.20658977329730988,\n",
       "  -0.18667899072170258,\n",
       "  -0.750108003616333,\n",
       "  -0.2633340358734131,\n",
       "  0.8838724493980408,\n",
       "  -0.03965182974934578,\n",
       "  -0.8446321487426758,\n",
       "  0.12205706536769867,\n",
       "  0.6193166375160217,\n",
       "  -0.413633793592453,\n",
       "  -0.49485087394714355,\n",
       "  0.2953401207923889,\n",
       "  0.23123903572559357,\n",
       "  -0.3912520706653595,\n",
       "  -0.26667073369026184,\n",
       "  0.2689085602760315,\n",
       "  0.08187285810709,\n",
       "  -0.36758774518966675,\n",
       "  -0.21452119946479797,\n",
       "  0.10153409093618393,\n",
       "  -0.034182511270046234,\n",
       "  -0.20018590986728668,\n",
       "  -0.06772904098033905,\n",
       "  -0.044024884700775146,\n",
       "  -0.15695147216320038,\n",
       "  -0.12065443396568298,\n",
       "  -0.009417061693966389,\n",
       "  -0.04439057037234306,\n",
       "  -0.1425682157278061,\n",
       "  -0.13311876356601715,\n",
       "  -0.04574926942586899,\n",
       "  -0.06645147502422333,\n",
       "  -0.10056319087743759,\n",
       "  -0.05829744040966034,\n",
       "  -0.04739072918891907,\n",
       "  -0.11098253726959229,\n",
       "  -0.1226571574807167,\n",
       "  -0.030414851382374763,\n",
       "  -0.051564771682024,\n",
       "  -0.02760823257267475,\n",
       "  -0.12163133174180984,\n",
       "  -0.10129499435424805,\n",
       "  0.007772886659950018,\n",
       "  -0.08837023377418518,\n",
       "  -0.08253613114356995,\n",
       "  -0.020149866119027138,\n",
       "  0.018626360222697258,\n",
       "  -0.12747591733932495,\n",
       "  -0.09144698828458786,\n",
       "  0.007910609245300293,\n",
       "  -0.02983907237648964,\n",
       "  -0.0969473198056221,\n",
       "  -0.04443087428808212,\n",
       "  -0.03240622207522392,\n",
       "  -0.09006310999393463,\n",
       "  -0.04141612350940704,\n",
       "  -0.01677747815847397,\n",
       "  -0.051847975701093674,\n",
       "  -0.08507876843214035,\n",
       "  -0.06429881602525711,\n",
       "  -0.06195949763059616,\n",
       "  -0.03345378860831261,\n",
       "  -0.09987705945968628,\n",
       "  -0.017422227188944817,\n",
       "  -0.003996651154011488,\n",
       "  -0.111447274684906,\n",
       "  -0.09033698588609695,\n",
       "  0.039313677698373795,\n",
       "  -0.05397490784525871,\n",
       "  -0.16641560196876526,\n",
       "  0.11642860621213913,\n",
       "  -0.09226971864700317,\n",
       "  -0.129702627658844,\n",
       "  -0.017396965995430946,\n",
       "  0.009032727219164371,\n",
       "  -0.11725270003080368,\n",
       "  -0.07343164831399918,\n",
       "  0.02066001109778881,\n",
       "  -0.02604764699935913,\n",
       "  0.08838795125484467,\n",
       "  0.06684481352567673,\n",
       "  -0.4354035556316376,\n",
       "  -1.0550410747528076,\n",
       "  0.44600632786750793,\n",
       "  0.5830327272415161,\n",
       "  -0.8107406497001648,\n",
       "  -0.4383498728275299,\n",
       "  0.6105484366416931,\n",
       "  0.1246337741613388,\n",
       "  -0.6502435803413391,\n",
       "  0.026322083547711372,\n",
       "  0.431702584028244,\n",
       "  -0.11239917576313019,\n",
       "  -0.4510761499404907,\n",
       "  0.02910967729985714,\n",
       "  0.4371628761291504,\n",
       "  -0.07716896384954453,\n",
       "  -0.5250572562217712,\n",
       "  0.24509555101394653,\n",
       "  -0.12701116502285004,\n",
       "  -0.05374139919877052,\n",
       "  -0.021774930879473686,\n",
       "  -0.07197213172912598,\n",
       "  0.04668945074081421,\n",
       "  -0.37472718954086304,\n",
       "  -0.029920626431703568,\n",
       "  0.04223693162202835,\n",
       "  0.12513978779315948,\n",
       "  -0.40105926990509033,\n",
       "  -0.06889508664608002,\n",
       "  0.10962288081645966,\n",
       "  -0.1865779161453247,\n",
       "  0.04729223996400833,\n",
       "  -0.22076307237148285,\n",
       "  0.0018509927904233336,\n",
       "  0.021150359883904457,\n",
       "  -0.31089019775390625,\n",
       "  0.14931996166706085,\n",
       "  -0.148496612906456,\n",
       "  0.0695614218711853,\n",
       "  -0.20512399077415466,\n",
       "  -0.04100631922483444,\n",
       "  -0.007309993728995323,\n",
       "  0.055195748805999756,\n",
       "  -0.23816841840744019,\n",
       "  -0.0390176959335804,\n",
       "  0.003740831511095166,\n",
       "  0.0791155993938446,\n",
       "  -0.28240615129470825,\n",
       "  0.051594920456409454,\n",
       "  -0.028753766790032387,\n",
       "  -0.1084856390953064,\n",
       "  0.006365545094013214,\n",
       "  -0.12828338146209717,\n",
       "  -0.10091572999954224,\n",
       "  -0.0525074377655983,\n",
       "  0.024434784427285194,\n",
       "  -0.09184673428535461,\n",
       "  -0.16466429829597473,\n",
       "  -0.025606365874409676,\n",
       "  -0.23149941861629486,\n",
       "  0.1317935585975647,\n",
       "  -0.1379694640636444,\n",
       "  -0.28873369097709656,\n",
       "  0.2115352600812912,\n",
       "  -0.18179930746555328,\n",
       "  -0.25854140520095825,\n",
       "  0.17699000239372253,\n",
       "  -0.27510866522789,\n",
       "  -0.042622316628694534,\n",
       "  -0.06762072443962097,\n",
       "  -0.14817450940608978,\n",
       "  -0.040181707590818405,\n",
       "  0.05191855505108833,\n",
       "  -0.48566412925720215,\n",
       "  0.2990294396877289,\n",
       "  -0.19565518200397491,\n",
       "  -0.3765004873275757,\n",
       "  0.21811601519584656,\n",
       "  -0.19551511108875275,\n",
       "  -0.10162533819675446,\n",
       "  -0.16528722643852234,\n",
       "  0.015766609460115433,\n",
       "  -0.11476446688175201,\n",
       "  -0.1877814382314682,\n",
       "  -0.15187975764274597,\n",
       "  0.05471484735608101,\n",
       "  -0.29578062891960144,\n",
       "  0.1774662733078003,\n",
       "  -0.5541598200798035,\n",
       "  0.2726963460445404,\n",
       "  -0.18017378449440002,\n",
       "  -0.38489431142807007,\n",
       "  0.3382835388183594,\n",
       "  -0.46830952167510986,\n",
       "  0.12760114669799805,\n",
       "  -0.30618393421173096,\n",
       "  -0.11637097597122192,\n",
       "  0.30432623624801636,\n",
       "  -0.5856302380561829,\n",
       "  -0.026369156315922737,\n",
       "  0.05411920323967934,\n",
       "  -0.24084392189979553,\n",
       "  -0.007569116074591875,\n",
       "  -0.2646555006504059,\n",
       "  0.15153264999389648,\n",
       "  -0.40371784567832947,\n",
       "  -0.20146577060222626,\n",
       "  0.48666009306907654,\n",
       "  -0.8024356961250305,\n",
       "  0.1816798746585846,\n",
       "  -0.10708989202976227,\n",
       "  -0.191336989402771,\n",
       "  0.16279326379299164,\n",
       "  -0.6165280342102051,\n",
       "  0.12345260381698608,\n",
       "  -0.06029019132256508,\n",
       "  0.05812626704573631,\n",
       "  -0.5969017744064331,\n",
       "  0.14744959771633148,\n",
       "  -0.14916694164276123,\n",
       "  -0.02989916503429413,\n",
       "  -0.1715262532234192,\n",
       "  -0.42102718353271484,\n",
       "  0.39495721459388733,\n",
       "  -0.6020749807357788,\n",
       "  0.03150134161114693,\n",
       "  0.06741274148225784,\n",
       "  -0.3609532415866852,\n",
       "  0.11209402233362198,\n",
       "  -0.5143790245056152,\n",
       "  0.19179801642894745,\n",
       "  -0.13200388848781586,\n",
       "  -0.1051759272813797,\n",
       "  -0.3794457018375397,\n",
       "  -0.03622017800807953,\n",
       "  0.5847930908203125,\n",
       "  -1.4106942415237427,\n",
       "  0.7692226767539978,\n",
       "  0.08645148575305939,\n",
       "  -0.9006115198135376,\n",
       "  0.13264751434326172,\n",
       "  0.04798275977373123,\n",
       "  0.0913785845041275,\n",
       "  -0.5408853888511658,\n",
       "  -0.12274263799190521,\n",
       "  0.4772292971611023,\n",
       "  -0.8037483096122742,\n",
       "  0.047370389103889465,\n",
       "  0.2185075879096985,\n",
       "  -0.3311825096607208,\n",
       "  -0.23555055260658264,\n",
       "  0.10829724371433258,\n",
       "  -0.12827281653881073,\n",
       "  -0.22117429971694946,\n",
       "  -0.16154766082763672,\n",
       "  0.3334142565727234,\n",
       "  -0.6202331781387329,\n",
       "  -0.1757098138332367,\n",
       "  0.7290045022964478,\n",
       "  -0.8958842158317566,\n",
       "  -0.1465875208377838,\n",
       "  0.3111325800418854,\n",
       "  -0.05661976709961891,\n",
       "  -0.4689570665359497,\n",
       "  0.05398135259747505,\n",
       "  -0.13659514486789703,\n",
       "  -0.1675111949443817,\n",
       "  0.32016706466674805,\n",
       "  -0.782642662525177,\n",
       "  0.3943970799446106,\n",
       "  -0.12347859144210815,\n",
       "  -0.41205012798309326,\n",
       "  0.15284110605716705,\n",
       "  -0.31071600317955017,\n",
       "  0.04487033188343048,\n",
       "  -0.10315945744514465,\n",
       "  -0.1821378618478775,\n",
       "  -0.07833945751190186,\n",
       "  -0.21402400732040405,\n",
       "  0.21146319806575775,\n",
       "  -0.2575230896472931,\n",
       "  -0.06762297451496124,\n",
       "  -0.1240546703338623,\n",
       "  -0.049179255962371826,\n",
       "  -0.16760528087615967,\n",
       "  0.30489474534988403,\n",
       "  -0.494547039270401,\n",
       "  -0.29646196961402893,\n",
       "  0.6161889433860779,\n",
       "  -0.37031441926956177,\n",
       "  -0.17680814862251282,\n",
       "  -0.20878992974758148,\n",
       "  -0.07799550145864487,\n",
       "  0.5551287531852722,\n",
       "  -0.6528348326683044,\n",
       "  -0.4203774333000183,\n",
       "  0.3276084363460541,\n",
       "  0.1991729736328125,\n",
       "  -0.40847328305244446,\n",
       "  -0.6415246725082397,\n",
       "  0.4614097476005554,\n",
       "  0.0443083830177784,\n",
       "  -0.32717642188072205,\n",
       "  0.027392525225877762,\n",
       "  -0.5953762531280518,\n",
       "  0.9013944268226624,\n",
       "  -0.8388450145721436,\n",
       "  -0.17410193383693695,\n",
       "  0.6301091909408569,\n",
       "  -0.5859948992729187,\n",
       "  -0.04921271651983261,\n",
       "  0.13030211627483368,\n",
       "  0.016106773167848587,\n",
       "  -0.6688474416732788,\n",
       "  0.6211159229278564,\n",
       "  -0.07702304422855377,\n",
       "  -0.5975754857063293,\n",
       "  0.10706810653209686,\n",
       "  0.4451413154602051,\n",
       "  -0.7425662875175476,\n",
       "  0.1285579651594162,\n",
       "  0.403128981590271,\n",
       "  -0.6347244381904602,\n",
       "  0.07236738502979279,\n",
       "  -0.15889066457748413,\n",
       "  0.13699603080749512,\n",
       "  0.023568594828248024,\n",
       "  -0.7401005029678345,\n",
       "  0.27374178171157837,\n",
       "  0.5697594285011292,\n",
       "  -0.8864830732345581,\n",
       "  0.454224556684494,\n",
       "  -0.5341699719429016,\n",
       "  -0.04305974394083023,\n",
       "  0.6366915702819824,\n",
       "  -0.587123692035675,\n",
       "  -0.20375235378742218,\n",
       "  0.2061145305633545,\n",
       "  -0.16924767196178436,\n",
       "  -0.03589224815368652,\n",
       "  0.2991979122161865,\n",
       "  -0.5695189833641052,\n",
       "  -0.45786306262016296,\n",
       "  0.880191445350647,\n",
       "  0.11078373342752457,\n",
       "  -1.1846559047698975,\n",
       "  0.1476486772298813,\n",
       "  0.5616481304168701,\n",
       "  -0.3942505121231079,\n",
       "  -0.3352978825569153,\n",
       "  -0.02900162898004055,\n",
       "  0.29424798488616943,\n",
       "  -0.4843691885471344,\n",
       "  0.204879492521286,\n",
       "  -0.10619264096021652,\n",
       "  -0.778985321521759,\n",
       "  0.9940427541732788,\n",
       "  -0.19102028012275696,\n",
       "  -0.9378876090049744,\n",
       "  0.5016170144081116,\n",
       "  -0.023497436195611954,\n",
       "  -0.1298213005065918,\n",
       "  0.241715207695961,\n",
       "  -0.5151398181915283,\n",
       "  -0.2551824748516083,\n",
       "  0.20348124206066132,\n",
       "  0.18988750874996185,\n",
       "  -0.30831873416900635,\n",
       "  -0.4506593942642212,\n",
       "  0.0771208330988884,\n",
       "  0.3186158835887909,\n",
       "  -0.45761165022850037,\n",
       "  -0.031128600239753723,\n",
       "  -0.020838521420955658,\n",
       "  -0.2689819931983948,\n",
       "  -0.270906001329422,\n",
       "  0.23382027447223663,\n",
       "  -0.09228477627038956,\n",
       "  -0.6561475396156311,\n",
       "  0.38730940222740173,\n",
       "  -0.18824589252471924,\n",
       "  -0.11886292695999146,\n",
       "  0.03635946288704872,\n",
       "  -0.5571392178535461,\n",
       "  0.47868725657463074,\n",
       "  -0.262466162443161,\n",
       "  0.022660136222839355,\n",
       "  -0.00020485602726694196,\n",
       "  0.015018447302281857,\n",
       "  -0.2870444357395172,\n",
       "  -0.4334360957145691,\n",
       "  0.7678728699684143,\n",
       "  -0.3950279653072357,\n",
       "  -0.40112635493278503,\n",
       "  0.30429184436798096,\n",
       "  -0.5843257904052734,\n",
       "  0.01425357535481453,\n",
       "  0.03582823649048805,\n",
       "  0.24273888766765594,\n",
       "  -0.4176420569419861,\n",
       "  -0.636488676071167,\n",
       "  0.9773804545402527,\n",
       "  -0.27490103244781494,\n",
       "  -0.5563542246818542,\n",
       "  -0.31389379501342773,\n",
       "  0.6059849262237549,\n",
       "  0.12183532863855362,\n",
       "  -0.7544989585876465,\n",
       "  0.1526973843574524,\n",
       "  0.3712603449821472,\n",
       "  -0.4684121012687683,\n",
       "  0.36261096596717834,\n",
       "  -0.27334359288215637,\n",
       "  -0.5788581967353821,\n",
       "  0.4839707612991333,\n",
       "  0.5138267874717712,\n",
       "  -0.7420949339866638,\n",
       "  -0.418325275182724,\n",
       "  0.32987356185913086,\n",
       "  0.4063010811805725,\n",
       "  -0.13690775632858276,\n",
       "  -0.8399165868759155,\n",
       "  0.2989315688610077,\n",
       "  0.9155603051185608,\n",
       "  -0.6729775667190552,\n",
       "  -0.7840390801429749,\n",
       "  0.9489393830299377,\n",
       "  0.013560066930949688,\n",
       "  -1.0120134353637695,\n",
       "  0.8026528358459473,\n",
       "  0.06554670631885529,\n",
       "  -0.8039427995681763,\n",
       "  0.2480114996433258,\n",
       "  0.07493403553962708,\n",
       "  -0.04785596579313278,\n",
       "  0.3774251937866211,\n",
       "  -0.9995501041412354,\n",
       "  0.23592345416545868,\n",
       "  0.8886871337890625,\n",
       "  -0.568393349647522,\n",
       "  -0.9754323363304138,\n",
       "  1.0329537391662598,\n",
       "  0.02149546891450882,\n",
       "  -0.8420162200927734,\n",
       "  0.8380905985832214,\n",
       "  -0.8450057506561279,\n",
       "  -0.08296221494674683,\n",
       "  0.33226293325424194,\n",
       "  0.3876800835132599,\n",
       "  -0.8554721474647522,\n",
       "  -0.12264814972877502,\n",
       "  0.4397353529930115,\n",
       "  -0.2463008463382721,\n",
       "  0.00030012655770406127,\n",
       "  -0.23619414865970612,\n",
       "  0.273253858089447,\n",
       "  0.03297891840338707,\n",
       "  -0.6172651648521423,\n",
       "  -0.09430302679538727,\n",
       "  1.253109335899353,\n",
       "  -0.9834532737731934,\n",
       "  -0.6732375621795654,\n",
       "  1.0639499425888062,\n",
       "  0.34515419602394104,\n",
       "  -1.0696183443069458,\n",
       "  -0.35273277759552,\n",
       "  1.2684836387634277,\n",
       "  -0.9113873839378357,\n",
       "  0.2239660918712616,\n",
       "  0.32262834906578064,\n",
       "  -0.22284160554409027,\n",
       "  -0.5080136060714722,\n",
       "  -0.09564486891031265,\n",
       "  0.9924733638763428,\n",
       "  -0.18342740833759308,\n",
       "  -1.2463138103485107,\n",
       "  0.07140995562076569,\n",
       "  1.6974238157272339,\n",
       "  -1.0156410932540894,\n",
       "  -0.8121482729911804,\n",
       "  1.5142782926559448,\n",
       "  -0.5806909799575806,\n",
       "  -0.6549082398414612,\n",
       "  -0.0694182738661766,\n",
       "  1.0315148830413818,\n",
       "  0.48306727409362793,\n",
       "  -0.9357578754425049,\n",
       "  -0.3330405056476593,\n",
       "  0.659182608127594,\n",
       "  0.3125971555709839,\n",
       "  -0.5150116086006165,\n",
       "  0.08348572254180908,\n",
       "  0.6286109089851379,\n",
       "  -0.2695789039134979,\n",
       "  0.007157042156904936,\n",
       "  -0.2594797909259796,\n",
       "  0.1911870688199997,\n",
       "  -0.32872575521469116,\n",
       "  1.1765305995941162,\n",
       "  0.029314858838915825,\n",
       "  -1.7781827449798584,\n",
       "  0.7068107724189758,\n",
       "  0.6749541759490967,\n",
       "  0.017706625163555145,\n",
       "  0.11777285486459732,\n",
       "  -0.6988419890403748,\n",
       "  0.4611484110355377,\n",
       "  0.4589610993862152,\n",
       "  -0.7242577075958252,\n",
       "  0.3997159004211426,\n",
       "  0.051278673112392426,\n",
       "  0.4377722144126892,\n",
       "  -0.6937508583068848,\n",
       "  -0.5390525460243225,\n",
       "  0.9224469065666199,\n",
       "  0.41045767068862915,\n",
       "  -0.21284537017345428,\n",
       "  -1.3490607738494873,\n",
       "  1.0629042387008667,\n",
       "  -0.16301657259464264,\n",
       "  -0.4183058440685272,\n",
       "  1.106859564781189,\n",
       "  -1.1371163129806519,\n",
       "  0.11355683952569962,\n",
       "  0.5997830033302307,\n",
       "  0.30056729912757874,\n",
       "  -0.6937049627304077,\n",
       "  -0.23445512354373932,\n",
       "  0.6914717555046082,\n",
       "  0.17642787098884583,\n",
       "  -0.10180892050266266,\n",
       "  -0.84000164270401,\n",
       "  1.1956441402435303,\n",
       "  -0.34533101320266724,\n",
       "  -0.3233587443828583,\n",
       "  0.6225510835647583,\n",
       "  -1.0172808170318604,\n",
       "  -0.009033664129674435,\n",
       "  1.4236373901367188,\n",
       "  0.6554839015007019,\n",
       "  -2.5504775047302246,\n",
       "  0.5924509763717651,\n",
       "  1.703892707824707,\n",
       "  -1.1683088541030884,\n",
       "  -0.3993808329105377,\n",
       "  0.6291642189025879,\n",
       "  -0.012666470371186733,\n",
       "  0.4195820391178131,\n",
       "  -1.0908522605895996,\n",
       "  0.3442879915237427,\n",
       "  1.1343436241149902,\n",
       "  -1.0198687314987183,\n",
       "  0.6160668730735779,\n",
       "  -0.633548378944397,\n",
       "  0.8956701159477234,\n",
       "  -0.46288833022117615,\n",
       "  -0.37888675928115845,\n",
       "  0.37784266471862793,\n",
       "  -0.49349796772003174,\n",
       "  0.28509122133255005,\n",
       "  0.40623268485069275,\n",
       "  -0.2464536875486374,\n",
       "  -0.08339236676692963,\n",
       "  -0.5416491031646729,\n",
       "  1.1710847616195679,\n",
       "  -0.23088277876377106,\n",
       "  -1.766445279121399,\n",
       "  1.9351013898849487,\n",
       "  -0.37895724177360535,\n",
       "  -0.01811695657670498,\n",
       "  0.13631412386894226,\n",
       "  -0.6132380962371826,\n",
       "  0.12195184826850891,\n",
       "  0.4254022538661957,\n",
       "  0.27843141555786133,\n",
       "  -0.5498204231262207,\n",
       "  -0.4679040312767029,\n",
       "  1.13754141330719,\n",
       "  -0.4816145896911621,\n",
       "  -0.42186078429222107,\n",
       "  0.13189451396465302,\n",
       "  0.3574346601963043,\n",
       "  0.6418216824531555,\n",
       "  -0.8786749839782715,\n",
       "  -0.7492328882217407,\n",
       "  2.1156375408172607,\n",
       "  -0.672419548034668,\n",
       "  -0.7335259914398193,\n",
       "  0.18888047337532043,\n",
       "  -0.5560999512672424,\n",
       "  0.5956995487213135,\n",
       "  0.3563491702079773,\n",
       "  0.11770975589752197,\n",
       "  -1.0432114601135254,\n",
       "  0.2201937884092331,\n",
       "  0.7117905616760254,\n",
       "  -0.9572805166244507,\n",
       "  1.1924320459365845,\n",
       "  0.1301639974117279,\n",
       "  -0.9077895283699036,\n",
       "  0.5483993291854858,\n",
       "  -0.19560210406780243,\n",
       "  0.39176809787750244,\n",
       "  -0.3347674310207367,\n",
       "  0.225081205368042,\n",
       "  -0.13605912029743195,\n",
       "  0.3171963095664978,\n",
       "  0.20616932213306427,\n",
       "  -0.40794315934181213,\n",
       "  -0.014781288802623749,\n",
       "  0.07010774314403534,\n",
       "  0.5592225193977356,\n",
       "  -0.138889342546463,\n",
       "  -0.8666306734085083,\n",
       "  0.5506341457366943,\n",
       "  0.31931793689727783,\n",
       "  -0.3062533736228943,\n",
       "  -0.14534278213977814,\n",
       "  0.09012507647275925,\n",
       "  0.3963385820388794,\n",
       "  0.31541359424591064,\n",
       "  -0.4568216800689697,\n",
       "  -0.24041341245174408,\n",
       "  -0.18864822387695312,\n",
       "  0.9406889081001282,\n",
       "  -0.16618476808071136,\n",
       "  -1.258346438407898,\n",
       "  0.7030096650123596,\n",
       "  0.4325805604457855,\n",
       "  -0.302682489156723,\n",
       "  -0.5358142852783203,\n",
       "  0.17149020731449127,\n",
       "  0.4243341386318207,\n",
       "  0.4152446389198303,\n",
       "  -0.4235483705997467,\n",
       "  -1.5117545127868652,\n",
       "  2.245561361312866,\n",
       "  -0.4805648624897003,\n",
       "  -1.4522148370742798,\n",
       "  1.2555902004241943,\n",
       "  -0.5607832670211792,\n",
       "  0.3274782598018646,\n",
       "  0.6724499464035034,\n",
       "  -0.7259478569030762,\n",
       "  -0.4041427671909332,\n",
       "  0.7271899580955505,\n",
       "  0.5895532965660095,\n",
       "  -0.30396878719329834,\n",
       "  -0.8682435750961304,\n",
       "  0.2310446947813034,\n",
       "  0.21480855345726013,\n",
       "  0.0698932632803917,\n",
       "  -0.39903101325035095,\n",
       "  -0.21023760735988617,\n",
       "  0.23003935813903809,\n",
       "  0.5962094664573669,\n",
       "  -0.24294660985469818,\n",
       "  -0.2933737337589264,\n",
       "  0.2695833742618561,\n",
       "  0.5458288788795471,\n",
       "  0.43500250577926636,\n",
       "  -0.1806974709033966,\n",
       "  -0.2569670081138611,\n",
       "  0.6736029982566833,\n",
       "  1.0679247379302979,\n",
       "  -0.6595800518989563,\n",
       "  -0.37541601061820984,\n",
       "  1.2324719429016113,\n",
       "  0.4444901645183563,\n",
       "  -0.029979871585965157,\n",
       "  -0.2537936866283417,\n",
       "  -0.2602493464946747,\n",
       "  1.009263277053833,\n",
       "  0.6510360836982727,\n",
       "  -1.6446553468704224,\n",
       "  -0.04660303145647049,\n",
       "  1.9607990980148315,\n",
       "  0.14981718361377716,\n",
       "  -0.7732077240943909,\n",
       "  -0.565842866897583,\n",
       "  0.6384682059288025,\n",
       "  0.7913491129875183,\n",
       "  -0.1760997474193573,\n",
       "  0.6109532117843628,\n",
       "  -1.0508462190628052,\n",
       "  -0.898020327091217,\n",
       "  1.4023181200027466,\n",
       "  0.5570500493049622,\n",
       "  -1.0361342430114746,\n",
       "  -0.17968571186065674,\n",
       "  0.4759683609008789,\n",
       "  0.31833434104919434,\n",
       "  -0.2716623544692993,\n",
       "  -0.7490383386611938,\n",
       "  0.4375894069671631,\n",
       "  0.36827537417411804,\n",
       "  -0.22357836365699768,\n",
       "  0.7050697803497314,\n",
       "  -0.2057332545518875,\n",
       "  -1.3365226984024048,\n",
       "  1.3189243078231812,\n",
       "  0.26169082522392273,\n",
       "  0.06771150231361389,\n",
       "  -0.36572182178497314,\n",
       "  -0.7066776156425476,\n",
       "  1.6505459547042847,\n",
       "  0.107061967253685,\n",
       "  -1.4103484153747559,\n",
       "  0.874539315700531,\n",
       "  0.1260414570569992,\n",
       "  -0.3968036472797394,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " 'labels': [3, 3, 3, 3, 3]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see dataset contents\n",
    "# something is wrong with the tokenizer\n",
    "processed_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>ppt_id</th>\n",
       "      <th>transcription</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>cb01</td>\n",
       "      <td>sun</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>lt04</td>\n",
       "      <td>chick</td>\n",
       "      <td>[3, 3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>cb10</td>\n",
       "      <td>sun</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>cb10</td>\n",
       "      <td>chick</td>\n",
       "      <td>[3, 3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>cb01</td>\n",
       "      <td>ramp</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>AM16</td>\n",
       "      <td>window</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>lm01</td>\n",
       "      <td>ship</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>pv04</td>\n",
       "      <td>sun</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>EB21</td>\n",
       "      <td>red</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>AM16</td>\n",
       "      <td>fox</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1192 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name ppt_id transcription  \\\n",
       "0     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   cb01           sun   \n",
       "1     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   lt04         chick   \n",
       "2     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   cb10           sun   \n",
       "3     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   cb10         chick   \n",
       "4     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   cb01          ramp   \n",
       "...                                                 ...    ...           ...   \n",
       "1187  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   AM16        window   \n",
       "1188  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   lm01          ship   \n",
       "1189  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   pv04           sun   \n",
       "1190  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   EB21           red   \n",
       "1191  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   AM16           fox   \n",
       "\n",
       "                  labels  \n",
       "0              [3, 3, 3]  \n",
       "1        [3, 3, 3, 3, 3]  \n",
       "2              [3, 3, 3]  \n",
       "3        [3, 3, 3, 3, 3]  \n",
       "4           [3, 3, 3, 3]  \n",
       "...                  ...  \n",
       "1187  [3, 3, 3, 3, 3, 3]  \n",
       "1188        [3, 3, 3, 3]  \n",
       "1189           [3, 3, 3]  \n",
       "1190           [3, 3, 3]  \n",
       "1191           [3, 3, 3]  \n",
       "\n",
       "[1192 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_transcription(text):\n",
    "    with processor.as_target_processor():\n",
    "        return processor.tokenizer(\n",
    "            text,\n",
    "            return_tensors=None,\n",
    "            padding=False,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "    \n",
    "df[\"labels\"] = df[\"transcription\"].apply(tokenize_transcription)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface transformers\n",
    "# Ensures that dataset is properly formatted for training\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Proceoutputs = model(input_values=batch[\"input_values\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n",
    "print(outputs.loss)  # This should show the loss during training\n",
    "ssor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_charsiu_finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1, # epochs\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=3, # k-fold = 8\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 10:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"./lora_charsiu_finetuned\")\n",
    "processor.save_pretrained(\"./lora_charsiu_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
