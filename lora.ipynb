{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import soundfile\n",
    "import random\n",
    "import pandas as pd\n",
    "# import issues here\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "# here\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Charsiu model\n",
    "charsiu_dir = '/home/cogsci-lasrlab/ForcedAlignment/programs/charsiu-main'\n",
    "os.chdir(charsiu_dir)\n",
    "\n",
    "sys.path.append('%s/src/' % charsiu_dir)\n",
    "\n",
    "from Charsiu import charsiu_forced_aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Load participant sets CSV\n",
    "participant_df = pd.read_csv(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/participant_sets.csv\")\n",
    "training_ids = participant_df[participant_df['set'].str.lower() == 'training']['ParticipantID'].str.lower().tolist()\n",
    "training_ids = [id.strip().lower() for id in training_ids]\n",
    "eval_ids = participant_df[participant_df['set'].str.lower() == 'evaluation']['ParticipantID'].str.lower().tolist()\n",
    "\n",
    "# Load csv of incorrect utterances\n",
    "# Create set of included files for training\n",
    "inclusion_df = pd.read_csv(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/incorrect_utterances.csv\")\n",
    "incorrect_word_files = inclusion_df[inclusion_df['Subject inclusion'].str.lower() == 'include']['Filename'].tolist()\n",
    "incorrect_word_files = set([f.lower() for f in incorrect_word_files])  # normalize case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728 total files.\n",
      "1591 files were included (both training and evaluation).\n",
      "1192 files are used for training.\n",
      "399 files are used fr evaluation.\n",
      "137 files were excluded due to incorrect word.\n"
     ]
    }
   ],
   "source": [
    "# extract filepath, participant info, and word from KT1 data\n",
    "base_dir = \"/home/cogsci-lasrlab/Documents/CSS_Capstone/KT1_data\"\n",
    "data = []\n",
    "included_count = 0\n",
    "excluded_count = 0\n",
    "num_files = 0\n",
    "num_train = 0\n",
    "num_eval = 0\n",
    "\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.wav') and \"participant_\" in file:\n",
    "                base = file[:-4]  # remove '.wav'\n",
    "                before, word = base.split(\"participant_\")\n",
    "                    \n",
    "                # Remove 'K1' and researcher number before 'participant_'\n",
    "                ppt_id = before.replace(\"K1\", \"\")[:-1]\n",
    "                ppt_id_clean = ppt_id.strip().lower() \n",
    "                \n",
    "                # handle missing participant\n",
    "                if \"rri0\" in file.lower():\n",
    "                    ppt_id_clean = \"rri0\"\n",
    "                num_files += 1\n",
    "\n",
    "                # Check if file has correct word\n",
    "                if file.lower() not in incorrect_word_files:\n",
    "                    included_count += 1\n",
    "                    # Only include training data\n",
    "                    if ppt_id_clean.lower() in training_ids:\n",
    "                        num_train += 1\n",
    "                        data.append({\n",
    "                            \"file_name\": os.path.join(folder_path, file),\n",
    "                            \"ppt_id\": ppt_id,\n",
    "                            \"transcription\": word\n",
    "                            })\n",
    "                            \n",
    "                            # Write .txt file\n",
    "#                             txt_path = os.path.join(folder_path, f\"{os.path.splitext(file)[0]}.txt\")\n",
    "#                             with open(txt_path, 'w') as f:\n",
    "#                                 f.write(word)\n",
    "                                \n",
    "                    #Count number of excluded file\n",
    "                    else:\n",
    "                        num_eval += 1\n",
    "                else:\n",
    "                    excluded_count += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Report\n",
    "print(f\"{num_files} total files.\") #this should be 1728\n",
    "print(f\"{included_count} files were included (both training and evaluation).\") # should be 1591\n",
    "print(f\"{num_train} files are used for training.\")\n",
    "print(f\"{num_eval} files are used fr evaluation.\")\n",
    "print(f\"{excluded_count} files were excluded due to incorrect word.\") # should be 137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Wav2Vec model and processor\n",
    "base_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 94,691,232 || trainable%: 0.3114\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA arguments\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "# Wrap the Wav2Vec2 model with LoRA\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(example[\"file_name\"])\n",
    "    \n",
    "    # Resample\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Flatten list of waveforms\n",
    "    waveform = waveform.squeeze(0)  # shape becomes [samples]\n",
    "\n",
    "    # Use the feature extractor (not full processor) — output is a dict with lists\n",
    "    inputs = processor.feature_extractor(\n",
    "        waveform,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=None  \n",
    "    )\n",
    "\n",
    "    # Flatten input_values from [[...]] → [...]\n",
    "    input_values = inputs[\"input_values\"][0]\n",
    "    attention_mask = inputs[\"attention_mask\"][0]\n",
    "\n",
    "    # Use the tokenizer for labels_ids (transcripts)\n",
    "    with processor.as_target_processor():\n",
    "        label_ids = processor.tokenizer(\n",
    "            example[\"transcription\"],\n",
    "            return_tensors=None,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "\n",
    "    return {\n",
    "        # keep all inputs lists\n",
    "        \"input_values\": input_values,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": label_ids  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1192 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1192/1192 [00:15<00:00, 74.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert data from pandas dataframe to transformers dataset\n",
    "# Preprocesses data for training\n",
    "dataset = Dataset.from_pandas(df)\n",
    "processed_dataset = dataset.map(preprocess, remove_columns=[\"file_name\", \"ppt_id\", \"transcription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface transformers\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Proceoutputs = model(input_values=batch[\"input_values\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n",
    "print(outputs.loss)  # This should show the loss during training\n",
    "ssor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_charsiu_finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1, # epochs\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=3, # k-fold = 8\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 10:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"./lora_charsiu_finetuned\")\n",
    "processor.save_pretrained(\"./lora_charsiu_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
