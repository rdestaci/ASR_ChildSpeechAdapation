{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# import soundfile\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Charsiu model\n",
    "charsiu_dir = '/home/cogsci-lasrlab/ForcedAlignment/programs/charsiu-main'\n",
    "os.chdir(charsiu_dir)\n",
    "\n",
    "sys.path.append('%s/src/' % charsiu_dir)\n",
    "\n",
    "from Charsiu import charsiu_forced_aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Load participant sets CSV\n",
    "participant_df = pd.read_csv(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/participant_sets.csv\")\n",
    "training_ids = participant_df[participant_df['set'].str.lower() == 'training']['ParticipantID'].str.lower().tolist()\n",
    "training_ids = [id.strip().lower() for id in training_ids]\n",
    "eval_ids = participant_df[participant_df['set'].str.lower() == 'evaluation']['ParticipantID'].str.lower().tolist()\n",
    "\n",
    "# Load csv of incorrect utterances\n",
    "# Create set of included files for training\n",
    "inclusion_df = pd.read_csv(\"/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/incorrect_utterances.csv\")\n",
    "incorrect_word_files = inclusion_df[inclusion_df['Subject inclusion'].str.lower() == 'include']['Filename'].tolist()\n",
    "incorrect_word_files = set([f.lower() for f in incorrect_word_files])  # normalize case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728 total files.\n",
      "1591 files were included (both training and evaluation).\n",
      "1192 files are used for training.\n",
      "399 files are used fr evaluation.\n",
      "137 files were excluded due to incorrect word.\n"
     ]
    }
   ],
   "source": [
    "# extract filepath, participant info, and word from KT1 data\n",
    "base_dir = \"/home/cogsci-lasrlab/Documents/CSS_Capstone/KT1_data\"\n",
    "data = []\n",
    "included_count = 0\n",
    "excluded_count = 0\n",
    "num_files = 0\n",
    "num_train = 0\n",
    "num_eval = 0\n",
    "\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.wav') and \"participant_\" in file:\n",
    "                base = file[:-4]  # remove '.wav'\n",
    "                before, word = base.split(\"participant_\")\n",
    "                    \n",
    "                # Remove 'K1' and researcher number before 'participant_'\n",
    "                ppt_id = before.replace(\"K1\", \"\")[:-1]\n",
    "                ppt_id_clean = ppt_id.strip().lower() \n",
    "                \n",
    "                # handle missing participant\n",
    "                if \"rri0\" in file.lower():\n",
    "                    ppt_id_clean = \"rri0\"\n",
    "                num_files += 1\n",
    "\n",
    "                # Check if file has correct word\n",
    "                if file.lower() not in incorrect_word_files:\n",
    "                    included_count += 1\n",
    "                    # Only include training data\n",
    "                    if ppt_id_clean.lower() in training_ids:\n",
    "                        num_train += 1\n",
    "                        data.append({\n",
    "                            \"file_name\": os.path.join(folder_path, file),\n",
    "                            \"ppt_id\": ppt_id,\n",
    "                            \"transcription\": word\n",
    "                            })\n",
    "                            \n",
    "                            # Write .txt file\n",
    "#                             txt_path = os.path.join(folder_path, f\"{os.path.splitext(file)[0]}.txt\")\n",
    "#                             with open(txt_path, 'w') as f:\n",
    "#                                 f.write(word)\n",
    "                                \n",
    "                    #Count number of excluded file\n",
    "                    else:\n",
    "                        num_eval += 1\n",
    "                else:\n",
    "                    excluded_count += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Report\n",
    "print(f\"{num_files} total files.\") # should be 1728\n",
    "print(f\"{included_count} files were included (both training and evaluation).\") # should be 1591\n",
    "print(f\"{num_train} files are used for training.\") # should be 1192\n",
    "print(f\"{num_eval} files are used fr evaluation.\") # should be 399\n",
    "print(f\"{excluded_count} files were excluded due to incorrect word.\") # should be 137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Wav2Vec model and processor\n",
    "base_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [11, 5], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer('HE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 94,691,232 || trainable%: 0.3114\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA arguments\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # can change rank, r=8 is most common\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # attention layers\n",
    ")\n",
    "\n",
    "# Add LoRA layers to Wav2Vec2 model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(example[\"file_name\"])\n",
    "    \n",
    "    # Resample\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Flatten list of waveforms for correct dimensionality\n",
    "    waveform = waveform.squeeze(0)  # shape becomes [samples]\n",
    "\n",
    "    # Use the feature extractor (not full processor) — output is a dict with lists\n",
    "    inputs = processor.feature_extractor(\n",
    "        waveform,\n",
    "        sampling_rate=16000,\n",
    "        padding=True, # ensures files are the same size\n",
    "        return_attention_mask=True, # tells the model what is actually audio and what is padding\n",
    "        return_tensors=None  \n",
    "    )\n",
    "\n",
    "    # Flatten input_values from [[...]] → [...] for correct dimensionality\n",
    "    input_values = inputs[\"input_values\"][0]\n",
    "    attention_mask = inputs[\"attention_mask\"][0]\n",
    "\n",
    "    # Tokenize word transcripts (label_ids)\n",
    "    with processor.as_target_processor():\n",
    "        label_ids = processor.tokenizer(\n",
    "            example[\"transcription\"].upper(), # upper to ensure correct tokenization\n",
    "            return_tensors=None,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "\n",
    "    return {\n",
    "        # keep all inputs lists\n",
    "        \"input_values\": input_values,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": label_ids  # something wrong here, tokenizing wrong\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1192 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1192/1192 [00:21<00:00, 54.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert data from pandas dataframe to transformers dataset\n",
    "# Preprocesses data for training\n",
    "dataset = Dataset.from_pandas(df)\n",
    "processed_dataset = dataset.map(preprocess, remove_columns=[\"file_name\", \"ppt_id\", \"transcription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>ppt_id</th>\n",
       "      <th>transcription</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>cb01</td>\n",
       "      <td>sun</td>\n",
       "      <td>[12, 16, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>lt04</td>\n",
       "      <td>chick</td>\n",
       "      <td>[19, 11, 10, 19, 26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>cb10</td>\n",
       "      <td>sun</td>\n",
       "      <td>[12, 16, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>cb10</td>\n",
       "      <td>chick</td>\n",
       "      <td>[19, 11, 10, 19, 26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>cb01</td>\n",
       "      <td>ramp</td>\n",
       "      <td>[13, 7, 17, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>AM16</td>\n",
       "      <td>window</td>\n",
       "      <td>[18, 10, 9, 14, 8, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>lm01</td>\n",
       "      <td>ship</td>\n",
       "      <td>[12, 11, 10, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>pv04</td>\n",
       "      <td>sun</td>\n",
       "      <td>[12, 16, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>EB21</td>\n",
       "      <td>red</td>\n",
       "      <td>[13, 5, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>/home/cogsci-lasrlab/Documents/CSS_Capstone/KT...</td>\n",
       "      <td>AM16</td>\n",
       "      <td>fox</td>\n",
       "      <td>[20, 8, 28]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1192 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name ppt_id transcription  \\\n",
       "0     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   cb01           sun   \n",
       "1     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   lt04         chick   \n",
       "2     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   cb10           sun   \n",
       "3     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   cb10         chick   \n",
       "4     /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   cb01          ramp   \n",
       "...                                                 ...    ...           ...   \n",
       "1187  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   AM16        window   \n",
       "1188  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   lm01          ship   \n",
       "1189  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   pv04           sun   \n",
       "1190  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   EB21           red   \n",
       "1191  /home/cogsci-lasrlab/Documents/CSS_Capstone/KT...   AM16           fox   \n",
       "\n",
       "                      labels  \n",
       "0                [12, 16, 9]  \n",
       "1       [19, 11, 10, 19, 26]  \n",
       "2                [12, 16, 9]  \n",
       "3       [19, 11, 10, 19, 26]  \n",
       "4            [13, 7, 17, 23]  \n",
       "...                      ...  \n",
       "1187  [18, 10, 9, 14, 8, 18]  \n",
       "1188        [12, 11, 10, 23]  \n",
       "1189             [12, 16, 9]  \n",
       "1190             [13, 5, 14]  \n",
       "1191             [20, 8, 28]  \n",
       "\n",
       "[1192 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_transcription(text):\n",
    "    with processor.as_target_processor():\n",
    "        return processor.tokenizer(\n",
    "            text.upper(),\n",
    "            return_tensors=None,\n",
    "            padding=False,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "    \n",
    "df[\"labels\"] = df[\"transcription\"].apply(tokenize_transcription)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface transformers\n",
    "# Ensures that dataset is properly formatted for training\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_charsiu_finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01, # L2 regularization\n",
    "    num_train_epochs=1, # make this larger\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=3, # experiment with making this larger (larger = faster)\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogsci-lasrlab/Documents/CSS_Capstone/lora_charsiu/.venv/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 10:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"./lora_charsiu_finetuned\")\n",
    "processor.save_pretrained(\"./lora_charsiu_finetuned\")\n",
    "# why is it not logging training loss now\n",
    "# where is this being saved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
